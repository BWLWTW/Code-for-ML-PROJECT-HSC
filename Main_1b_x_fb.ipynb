{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc844959-deb0-4ac0-be9b-cb18ac585544",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''pip install numpy==1.23.5\n",
    "pip install pandas==2.0.3\n",
    "pip install keras==2.3.0\n",
    "pip install tensorflow==2.4.3'''\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import os\n",
    "import random\n",
    "# import ML related libraries\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# import plotting related libraries\n",
    "from scipy import interpolate\n",
    "from keras.layers import LeakyReLU\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "731298ab-dd23-4629-8aa8-4c71ae20895e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    '''\n",
    "    Plot learning curve using NN training history info\n",
    "    '''\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.xlabel('Epoch',fontsize=4)\n",
    "    plt.ylabel('Mean Square Error',fontsize=4)\n",
    "    plt.plot(hist['epoch'], hist['val_mse'],\n",
    "            label = 'Val Error')\n",
    "    plt.plot(hist['epoch'], hist['mse'],\n",
    "            label='Train Error')\n",
    "    plt.ylim([0,10*10**-3])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "def data_import(csv_file_name,input_column_list,output_column_list):\n",
    "    '''\n",
    "    used to import dataset and split training and test dataset\n",
    "    csv_file_name is the dateset root\n",
    "    reture traing and test datasets\n",
    "    '''\n",
    "    raw_data = pd.read_csv(csv_file_name,header=None).dropna()\n",
    "    x = raw_data.iloc[:,input_column_list].astype(float)\n",
    "    y = raw_data.iloc[:,output_column_list].astype(float)\n",
    "    return x, y\n",
    "def data_melt(x, y,xcolumn,ycolumn,yparameter,yname):\n",
    "    '''\n",
    "    Add position as input\n",
    "    input:\n",
    "        x: old input \n",
    "        y: old output\n",
    "    output:\n",
    "        x_new: new input \n",
    "        y_new: new output\n",
    "    '''\n",
    "    y_label_str = [str(x) for x in ycolumn] # make a list of string of y_label_new\n",
    "    dataset = pd.concat([x, y],axis=1, ignore_index=True)\n",
    "    col_names = xcolumn + y_label_str\n",
    "    dataset.columns = col_names\n",
    "    dataset = dataset.melt(id_vars=xcolumn, \n",
    "        var_name=yparameter, \n",
    "        value_name=yname) #for given case, select which HSP to be found\n",
    "    x_new = dataset.iloc[:, 0:(len(xcolumn)+1)]\n",
    "    y_new = dataset.iloc[:, (len(xcolumn)+1)]\n",
    "    print('DATASET')\n",
    "    print(dataset)\n",
    "    return x_new, y_new\n",
    "def MSE(testY, predicY):\n",
    "    '''\n",
    "    Get MSE fun\n",
    "    '''\n",
    "    MSE=np.sum(np.power((testY - predicY),2))/testY.shape[1]/testY.shape[0]\n",
    "    return MSE\n",
    "def get_N_output(Y_train):\n",
    "    '''\n",
    "    Get the number of outputs\n",
    "    '''\n",
    "    if Y_train.ndim == 1:\n",
    "        N_outputs = 1\n",
    "    else:\n",
    "        N_outputs = Y_train.shape[1]\n",
    "    return N_outputs\n",
    "def print_result(Y_test,predict_test_scal,predict_test,scaler_Y,history):\n",
    "    Y_test_scal = scaler_Y.transform(Y_test.values.reshape(-1,1)).reshape(-1, get_N_output(Y_test))\n",
    "    MSE_R_scal = MSE(Y_test_scal, predict_test_scal)\n",
    "    MSE_R = MSE(Y_test.values, predict_test)\n",
    "    print('MSE_R_scal:')\n",
    "    print(MSE_R_scal)\n",
    "    print('MSE_R:')\n",
    "    print(MSE_R)\n",
    "    plot_history(history)\n",
    "    pred_results = []\n",
    "    sim_results = []\n",
    "    for i in range(len(predict_test)):\n",
    "        for j in range(len(predict_test[0])):\n",
    "            sim_results.append(Y_test.iloc[i, j])\n",
    "            pred_results.append(predict_test[i][j])\n",
    "    max_results = max(max(pred_results,sim_results))\n",
    "    min_results = min(min(pred_results,sim_results))\n",
    "    fig,ax = plt.subplots(figsize=(5,5))\n",
    "    ax.plot([min_results, max_results], [min_results, max_results],'b', linewidth=3)\n",
    "    ax.scatter(pred_results, sim_results,s=10, c='r')\n",
    "    ax.set_xlabel(\"Prediction\", fontsize=4)\n",
    "    ax.set_ylabel(\"FEM testing value\", fontsize=4)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "def pre_processing(model_type, x, y,train_test_split_test_size,xcolumn,ycolumn,yparameter,yname):\n",
    "    '''\n",
    "    Data preprocessing (Uniformly spaced sampling, normalisation, train test split)\n",
    "    inputs:\n",
    "        model_type: 'ANN1' the first architecture (3 inputs and 21 outputs)\n",
    "                    'ANN2' the second architecture (4 inputs and 1 output)        \n",
    "        x: a dataframe of inputs of the whole dataset\n",
    "        y: a dataframe of outputs of the whole dataset\n",
    "    outputs:\n",
    "        model_type: 'ANN1' the first architecture (3 inputs and 21 outputs)\n",
    "                    'ANN2' the second architecture (4 inputs and 1 output)\n",
    "        Proc_X_train: Processed Training input\n",
    "        Proc_Y_train: Processed Training output\n",
    "        Proc_X_test: Processed Test input\n",
    "        Y_test: Test output\n",
    "    '''\n",
    "    # Split dataset\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(x,y,test_size=train_test_split_test_size, random_state=3)\n",
    "    X_train=pd.DataFrame(X_train[:].values)\n",
    "    X_test =pd.DataFrame(X_test[:].values)\n",
    "    Y_train=pd.DataFrame(Y_train[:].values)\n",
    "    Y_test=pd.DataFrame(Y_test[:].values)\n",
    "    # data reconstruction\n",
    "    if 'ANN2' in model_type:\n",
    "        X_train, Y_train = data_melt(X_train, Y_train,xcolumn,ycolumn,yparameter,yname)\n",
    "        X_test, Y_test_1 = data_melt(X_test, Y_test,xcolumn,ycolumn,yparameter,yname)\n",
    "\n",
    "    # Normalization\n",
    "    global scaler_X, scaler_Y\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_Y = StandardScaler()\n",
    "    scaled_train_X = scaler_X.fit_transform(X_train.to_numpy())\n",
    "    scaled_test_X = scaler_X.transform(X_test)\n",
    "\n",
    "    if Y_train.ndim == 1:\n",
    "      Y_train = np.array(Y_train).reshape(-1,1)#column array\n",
    "    scaled_train_Y = scaler_Y.fit_transform(Y_train)\n",
    "    Proc_X_train = scaled_train_X\n",
    "    Proc_Y_train = scaled_train_Y\n",
    "    Proc_X_test = scaled_test_X\n",
    "\n",
    "    return model_type, Proc_X_train, Proc_Y_train, Proc_X_test, Y_test\n",
    "def seed_tensorflow(seed):\n",
    "    '''\n",
    "    Fix ramdom seed\n",
    "    '''\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "def save_model(model_path,ANN):\n",
    "    '''save the model'''\n",
    "    ANN.save(model_path)\n",
    "    print('done saving model')\n",
    "def build_model_SGD(Proc_X_train, Proc_Y_train,N_hidden_nodes,learning_rate,momentum, nesterov,Batch_size, Epochs,BEST_ACT_1,validation_split):\n",
    "    '''\n",
    "    Build a NN\n",
    "    inputs:\n",
    "        N_hidden_nodes: number of neurons\n",
    "        input_dim: number of inputs\n",
    "        N_outputs: number of outputs\n",
    "        l_rate: learning rate\n",
    "        Batch_size: Batch size\n",
    "    outputs:\n",
    "        model: Trained model\n",
    "        history: training history\n",
    "    '''\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_mse', patience=100, verbose=1),\n",
    "        ModelCheckpoint('best_model.h5', monitor='val_mse', save_best_only=True, verbose=0)\n",
    "    ]\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(N_hidden_nodes, activation=BEST_ACT_1, input_shape=(Proc_X_train.shape[1],)),\n",
    "        keras.layers.Dense(get_N_output(Proc_Y_train),activation='linear')\n",
    "    ])\n",
    "    model.compile(loss='mse',\n",
    "                optimizer=keras.optimizers.SGD(learning_rate=learning_rate,momentum=momentum,nesterov=nesterov),\n",
    "                metrics=['mse'])\n",
    "    \n",
    "    history = model.fit(Proc_X_train, Proc_Y_train, batch_size=Batch_size, epochs=Epochs, \n",
    "                        verbose=1, validation_split=validation_split, callbacks=callbacks)\n",
    "    model.summary()\n",
    "    return model, history\n",
    "    \n",
    "def build_model_RMSprop(Proc_X_train, Proc_Y_train,N_hidden_nodes,learning_rate,rho,momentum,epsilon,Batch_size, Epochs,BEST_ACT_1,validation_split):\n",
    "    '''\n",
    "    Build a NN\n",
    "    inputs:\n",
    "        N_hidden_nodes: number of neurons\n",
    "        input_dim: number of inputs\n",
    "        N_outputs: number of outputs\n",
    "        l_rate: learning rate\n",
    "        Batch_size: Batch size\n",
    "    outputs:\n",
    "        model: Trained model\n",
    "        history: training history\n",
    "    '''\n",
    "    callbacks = [EarlyStopping(monitor='val_mse', patience=100, verbose=1),ModelCheckpoint('best_model.h5', monitor='val_mse', save_best_only=True, verbose=1)]\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(N_hidden_nodes, activation=BEST_ACT_1, input_shape=(Proc_X_train.shape[1],)),\n",
    "        keras.layers.Dense(get_N_output(Proc_Y_train),activation='linear')\n",
    "    ])\n",
    "    model.compile(loss='mse',\n",
    "                optimizer=keras.optimizers.RMSprop(learning_rate=learning_rate,rho=rho,momentum=momentum,epsilon=epsilon),\n",
    "                metrics=['mse'])\n",
    "    model.summary()\n",
    "    history = model.fit(Proc_X_train, Proc_Y_train, batch_size=Batch_size, epochs=Epochs, \n",
    "                        verbose=1, validation_split=validation_split, callbacks=callbacks)\n",
    "    return model, history\n",
    "def build_model_Adagrad(Proc_X_train, Proc_Y_train,N_hidden_nodes,learning_rate,initial_accumulator_value, epsilon,Batch_size, Epochs,BEST_ACT_1,validation_split):\n",
    "    '''\n",
    "    Build a NN\n",
    "    inputs:\n",
    "        N_hidden_nodes: number of neurons\n",
    "        input_dim: number of inputs\n",
    "        N_outputs: number of outputs\n",
    "        l_rate: learning rate\n",
    "        Batch_size: Batch size\n",
    "    outputs:\n",
    "        model: Trained model\n",
    "        history: training history\n",
    "    '''\n",
    "    callbacks = [EarlyStopping(monitor='val_mse', patience=100, verbose=1),ModelCheckpoint('best_model.h5', monitor='val_mse', save_best_only=True, verbose=1)]\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(N_hidden_nodes, activation=BEST_ACT_1, input_shape=(Proc_X_train.shape[1],)),\n",
    "        keras.layers.Dense(get_N_output(Proc_Y_train),activation='linear')\n",
    "    ])\n",
    "    model.compile(loss='mse',\n",
    "                  optimizer=keras.optimizers.Adagrad(learning_rate=learning_rate,initial_accumulator_value=initial_accumulator_value,epsilon=epsilon),\n",
    "                  metrics=['mse'])\n",
    "    model.summary()\n",
    "    history = model.fit(Proc_X_train, Proc_Y_train, batch_size=Batch_size, epochs=Epochs, \n",
    "                        verbose=1, validation_split=validation_split, callbacks=callbacks)\n",
    "    return model, history\n",
    "def build_model_Adadelta(Proc_X_train, Proc_Y_train,N_hidden_nodes,learning_rate,rho, epsilon,Batch_size, Epochs,BEST_ACT_1,validation_split):\n",
    "    '''\n",
    "    Build a NN\n",
    "    inputs:\n",
    "        N_hidden_nodes: number of neurons\n",
    "        input_dim: number of inputs\n",
    "        N_outputs: number of outputs\n",
    "        l_rate: learning rate\n",
    "        Batch_size: Batch size\n",
    "    outputs:\n",
    "        model: Trained model\n",
    "        history: training history\n",
    "    '''\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_mse', patience=100, verbose=1),\n",
    "        ModelCheckpoint('best_model.h5', monitor='val_mse', save_best_only=True, verbose=1)\n",
    "    ]\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(N_hidden_nodes, activation=BEST_ACT_1, input_shape=(Proc_X_train.shape[1],)),\n",
    "        keras.layers.Dense(get_N_output(Proc_Y_train),activation='linear')\n",
    "    ])\n",
    "    model.compile(loss='mse',\n",
    "                  optimizer=keras.optimizers.Adadelta(learning_rate=learning_rate,rho=rho,epsilon=epsilon),\n",
    "                  metrics=['mse'])\n",
    "    model.summary()\n",
    "    history = model.fit(Proc_X_train, Proc_Y_train, batch_size=Batch_size, epochs=Epochs, \n",
    "                        verbose=1, validation_split=validation_split, callbacks=callbacks)\n",
    "    return model, history\n",
    "def build_model_Adam(Proc_X_train, Proc_Y_train,N_hidden_nodes,learning_rate,beta_1,beta_2, epsilon,Batch_size, Epochs,BEST_ACT_1,validation_split):\n",
    "    '''\n",
    "    Build a NN\n",
    "    inputs:\n",
    "        N_hidden_nodes: number of neurons\n",
    "        input_dim: number of inputs\n",
    "        N_outputs: number of outputs\n",
    "        l_rate: learning rate\n",
    "        Batch_size: Batch size\n",
    "    outputs:\n",
    "        model: Trained model\n",
    "        history: training history\n",
    "    '''\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_mse', patience=100, verbose=1),\n",
    "        ModelCheckpoint('best_model.h5', monitor='val_mse', save_best_only=True, verbose=0)\n",
    "    ]\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(N_hidden_nodes, activation=BEST_ACT_1, input_shape=(Proc_X_train.shape[1],)),\n",
    "        keras.layers.Dense(get_N_output(Proc_Y_train),activation='linear')\n",
    "    ])\n",
    "    model.compile(loss='mse',\n",
    "                  optimizer=keras.optimizers.Adam(learning_rate=learning_rate,beta_1=beta_1,beta_2=beta_2,epsilon=epsilon),\n",
    "                  metrics=['mse'])\n",
    "    model.summary()\n",
    "    history = model.fit(Proc_X_train, Proc_Y_train, batch_size=Batch_size, epochs=Epochs, \n",
    "                        verbose=1, validation_split=validation_split, callbacks=callbacks)\n",
    "    return model, history\n",
    "def build_model_Adamax(Proc_X_train, Proc_Y_train,N_hidden_nodes,learning_rate,beta_1,beta_2, epsilon,Batch_size, Epochs,BEST_ACT_1,validation_split):\n",
    "    '''\n",
    "    Build a NN\n",
    "    inputs:\n",
    "        N_hidden_nodes: number of neurons\n",
    "        input_dim: number of inputs\n",
    "        N_outputs: number of outputs\n",
    "        l_rate: learning rate\n",
    "        Batch_size: Batch size\n",
    "    outputs:\n",
    "        model: Trained model\n",
    "        history: training history\n",
    "    '''\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_mse', patience=100, verbose=1),\n",
    "        ModelCheckpoint('best_model.h5', monitor='val_mse', save_best_only=True, verbose=0)\n",
    "    ]\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(N_hidden_nodes, activation=BEST_ACT_1, input_shape=(Proc_X_train.shape[1],)),\n",
    "        keras.layers.Dense(get_N_output(Proc_Y_train),activation='linear')\n",
    "    ])\n",
    "    model.compile(loss='mse',\n",
    "                  optimizer=keras.optimizers.Adamax(learning_rate=learning_rate,beta_1=beta_1,beta_2=beta_2,epsilon=epsilon),\n",
    "                  metrics=['mse'])\n",
    "    model.summary()\n",
    "    history = model.fit(Proc_X_train, Proc_Y_train, batch_size=Batch_size, epochs=Epochs, \n",
    "                        verbose=1, validation_split=validation_split, callbacks=callbacks)\n",
    "    return model, history\n",
    "def build_model_NAdam(Proc_X_train, Proc_Y_train,N_hidden_nodes,learning_rate,beta_1,beta_2, epsilon,Batch_size, Epochs,BEST_ACT_1,validation_split):\n",
    "    '''\n",
    "    Build a NN\n",
    "    inputs:\n",
    "        N_hidden_nodes: number of neurons\n",
    "        input_dim: number of inputs\n",
    "        N_outputs: number of outputs\n",
    "        l_rate: learning rate\n",
    "        Batch_size: Batch size\n",
    "    outputs:\n",
    "        model: Trained model\n",
    "        history: training history\n",
    "    '''\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_mse', patience=100, verbose=1),\n",
    "        ModelCheckpoint('best_model.h5', monitor='val_mse', save_best_only=True, verbose=0)\n",
    "    ]\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(N_hidden_nodes, activation=BEST_ACT_1, input_shape=(Proc_X_train.shape[1],)),\n",
    "        keras.layers.Dense(get_N_output(Proc_Y_train),activation='linear')\n",
    "    ])\n",
    "    model.compile(loss='mse',\n",
    "                  optimizer=keras.optimizers.Nadam(learning_rate=learning_rate,beta_1=beta_1,beta_2=beta_2,epsilon=epsilon),\n",
    "                  metrics=['mse'])\n",
    "    model.summary()\n",
    "    history = model.fit(Proc_X_train, Proc_Y_train, batch_size=Batch_size, epochs=Epochs, \n",
    "                        verbose=1, validation_split=validation_split, callbacks=callbacks)\n",
    "    return model, history\n",
    "def get_result_SGD(model_type, Proc_X_train, Proc_Y_train, Proc_X_test,Y_test, N_hidden_nodes,learning_rate,momentum, nesterov,Batch_size, Epochs,BEST_ACT_1,validation_split):\n",
    "    '''\n",
    "    Get the trained model, prediction results, and NN training history\n",
    "    inputs:\n",
    "        model_type: 'ANN1' the first architecture (3 inputs and 21 outputs)\n",
    "                    'ANN2' the second architecture (4 inputs and 1 output)\n",
    "        Proc_X_train: Processed Training input\n",
    "        Proc_Y_train: Processed Training output\n",
    "        Proc_X_test: Processed Test input\n",
    "        N_neurons: Number of Neurons in the hidden layer\n",
    "        l_rate: Learning rate\n",
    "    outputs:\n",
    "        model: Trained model\n",
    "        predict_test: predicted results\n",
    "        history: training history\n",
    "    '''\n",
    "    #build model\n",
    "    model, history = build_model_SGD(Proc_X_train, Proc_Y_train,N_hidden_nodes,learning_rate,momentum, nesterov,Batch_size, Epochs,BEST_ACT_1,validation_split)\n",
    "    # Predict\n",
    "    \n",
    "    predict_test_scal = model.predict(Proc_X_test)\n",
    "    if 'ANN2' in model_type:\n",
    "      predict_test_scal = np.reshape(predict_test_scal, (get_N_output(Y_test), -1)).T\n",
    "    \n",
    "    predict_test = scaler_Y.inverse_transform(predict_test_scal)\n",
    "    return  model, predict_test_scal, predict_test, history\n",
    "\n",
    "\n",
    "def get_result_RMSprop(model_type, Proc_X_train, Proc_Y_train, Proc_X_test,Y_test,N_hidden_nodes,learning_rate,rho,momentum, epsilon,Batch_size, Epochs,BEST_ACT_1,validation_split):\n",
    "    '''\n",
    "    Get the trained model, prediction results, and NN training history\n",
    "    inputs:\n",
    "        model_type: 'ANN1' the first architecture (3 inputs and 21 outputs)\n",
    "                    'ANN2' the second architecture (4 inputs and 1 output)\n",
    "        Proc_X_train: Processed Training input\n",
    "        Proc_Y_train: Processed Training output\n",
    "        Proc_X_test: Processed Test input\n",
    "        N_neurons: Number of Neurons in the hidden layer\n",
    "        l_rate: Learning rate\n",
    "    outputs:\n",
    "        model: Trained model\n",
    "        predict_test: predicted results\n",
    "        history: training history\n",
    "    '''\n",
    "    #build model\n",
    "    model, history = build_model_RMSprop(Proc_X_train, Proc_Y_train,N_hidden_nodes,learning_rate,rho,momentum, epsilon,Batch_size, Epochs,BEST_ACT_1,validation_split)\n",
    "    # Predict\n",
    "    \n",
    "    predict_test_scal = model.predict(Proc_X_test)\n",
    "    if 'ANN2' in model_type:\n",
    "      predict_test_scal = np.reshape(predict_test_scal, (get_N_output(Y_test), -1)).T\n",
    "    \n",
    "    predict_test = scaler_Y.inverse_transform(predict_test_scal)\n",
    "    return  model, predict_test_scal, predict_test, history\n",
    "\n",
    "def get_result_Adagrad(model_type, Proc_X_train, Proc_Y_train, Proc_X_test,Y_test,N_hidden_nodes,learning_rate,initial_accumulator_value, epsilon,Batch_size, Epochs,BEST_ACT_1,validation_split):\n",
    "    '''\n",
    "    Get the trained model, prediction results, and NN training history\n",
    "    inputs:\n",
    "        model_type: 'ANN1' the first architecture (3 inputs and 21 outputs)\n",
    "                    'ANN2' the second architecture (4 inputs and 1 output)\n",
    "        Proc_X_train: Processed Training input\n",
    "        Proc_Y_train: Processed Training output\n",
    "        Proc_X_test: Processed Test input\n",
    "        N_neurons: Number of Neurons in the hidden layer\n",
    "        l_rate: Learning rate\n",
    "    outputs:\n",
    "        model: Trained model\n",
    "        predict_test: predicted results\n",
    "        history: training history\n",
    "    '''\n",
    "    #build model\n",
    "    model, history = build_model_Adagrad(Proc_X_train, Proc_Y_train,N_hidden_nodes,learning_rate,initial_accumulator_value, epsilon,Batch_size, Epochs,BEST_ACT_1,validation_split)\n",
    "    # Predict\n",
    "    \n",
    "    predict_test_scal = model.predict(Proc_X_test)\n",
    "    if 'ANN2' in model_type:\n",
    "      predict_test_scal = np.reshape(predict_test_scal, (get_N_output(Y_test), -1)).T\n",
    "    \n",
    "    predict_test = scaler_Y.inverse_transform(predict_test_scal)\n",
    "    return  model, predict_test_scal, predict_test, history\n",
    "\n",
    "def get_result_Adadelta(model_type, Proc_X_train, Proc_Y_train, Proc_X_test,Y_test,N_hidden_nodes,learning_rate,rho, epsilon,Batch_size, Epochs,BEST_ACT_1,validation_split):\n",
    "    '''\n",
    "    Get the trained model, prediction results, and NN training history\n",
    "    inputs:\n",
    "        model_type: 'ANN1' the first architecture (3 inputs and 21 outputs)\n",
    "                    'ANN2' the second architecture (4 inputs and 1 output)\n",
    "        Proc_X_train: Processed Training input\n",
    "        Proc_Y_train: Processed Training output\n",
    "        Proc_X_test: Processed Test input\n",
    "        N_neurons: Number of Neurons in the hidden layer\n",
    "        l_rate: Learning rate\n",
    "    outputs:\n",
    "        model: Trained model\n",
    "        predict_test: predicted results\n",
    "        history: training history\n",
    "    '''\n",
    "    #build model\n",
    "    model, history = build_model_Adadelta(Proc_X_train, Proc_Y_train,N_hidden_nodes,learning_rate,rho, epsilon,Batch_size, Epochs,BEST_ACT_1,validation_split)\n",
    "    # Predict\n",
    "    \n",
    "    predict_test_scal = model.predict(Proc_X_test)\n",
    "    if 'ANN2' in model_type:\n",
    "      predict_test_scal = np.reshape(predict_test_scal, (get_N_output(Y_test), -1)).T\n",
    "    \n",
    "    predict_test = scaler_Y.inverse_transform(predict_test_scal)\n",
    "    return  model, predict_test_scal, predict_test, history\n",
    "\n",
    "def get_result_Adam(model_type,Proc_X_train, Proc_Y_train, Proc_X_test,Y_test,N_hidden_nodes,learning_rate,beta_1,beta_2, epsilon,Batch_size, Epochs,BEST_ACT_1,validation_split):\n",
    "    '''\n",
    "    Get the trained model, prediction results, and NN training history\n",
    "    inputs:\n",
    "        model_type: 'ANN1' the first architecture (3 inputs and 21 outputs)\n",
    "                    'ANN2' the second architecture (4 inputs and 1 output)\n",
    "        Proc_X_train: Processed Training input\n",
    "        Proc_Y_train: Processed Training output\n",
    "        Proc_X_test: Processed Test input\n",
    "        N_neurons: Number of Neurons in the hidden layer\n",
    "        l_rate: Learning rate\n",
    "    outputs:\n",
    "        model: Trained model\n",
    "        predict_test: predicted results\n",
    "        history: training history\n",
    "    '''\n",
    "    #build model\n",
    "    model, history = build_model_Adam(Proc_X_train, Proc_Y_train,N_hidden_nodes,learning_rate,beta_1,beta_2, epsilon,Batch_size, Epochs,BEST_ACT_1,validation_split)\n",
    "    # Predict\n",
    "    \n",
    "    predict_test_scal = model.predict(Proc_X_test)\n",
    "    if 'ANN2' in model_type:\n",
    "      predict_test_scal = np.reshape(predict_test_scal, (get_N_output(Y_test), -1)).T\n",
    "    \n",
    "    predict_test = scaler_Y.inverse_transform(predict_test_scal)\n",
    "    return  model, predict_test_scal, predict_test, history\n",
    "\n",
    "def get_result_Adamax(model_type, Proc_X_train, Proc_Y_train, Proc_X_test,Y_test,N_hidden_nodes,learning_rate,beta_1,beta_2, epsilon,Batch_size, Epochs,BEST_ACT_1,validation_split):\n",
    "    '''\n",
    "    Get the trained model, prediction results, and NN training history\n",
    "    inputs:\n",
    "        model_type: 'ANN1' the first architecture (3 inputs and 21 outputs)\n",
    "                    'ANN2' the second architecture (4 inputs and 1 output)\n",
    "        Proc_X_train: Processed Training input\n",
    "        Proc_Y_train: Processed Training output\n",
    "        Proc_X_test: Processed Test input\n",
    "        N_neurons: Number of Neurons in the hidden layer\n",
    "        l_rate: Learning rate\n",
    "    outputs:\n",
    "        model: Trained model\n",
    "        predict_test: predicted results\n",
    "        history: training history\n",
    "    '''\n",
    "    #build model\n",
    "    model, history = build_model_Adamax(Proc_X_train, Proc_Y_train,N_hidden_nodes,learning_rate,beta_1,beta_2, epsilon,Batch_size, Epochs,BEST_ACT_1,validation_split)\n",
    "    # Predict\n",
    "    \n",
    "    predict_test_scal = model.predict(Proc_X_test)\n",
    "    if 'ANN2' in model_type:\n",
    "      predict_test_scal = np.reshape(predict_test_scal, (get_N_output(Y_test), -1)).T\n",
    "    \n",
    "    predict_test = scaler_Y.inverse_transform(predict_test_scal)\n",
    "    return  model, predict_test_scal, predict_test, history\n",
    "\n",
    "def get_result_NAdam(model_type, Proc_X_train, Proc_Y_train, Proc_X_test,Y_test,N_hidden_nodes,learning_rate,beta_1,beta_2, epsilon,Batch_size, Epochs,BEST_ACT_1,validation_split):\n",
    "    '''\n",
    "    Get the trained model, prediction results, and NN training history\n",
    "    inputs:\n",
    "        model_type: 'ANN1' the first architecture (3 inputs and 21 outputs)\n",
    "                    'ANN2' the second architecture (4 inputs and 1 output)\n",
    "        Proc_X_train: Processed Training input\n",
    "        Proc_Y_train: Processed Training output\n",
    "        Proc_X_test: Processed Test input\n",
    "        N_neurons: Number of Neurons in the hidden layer\n",
    "        l_rate: Learning rate\n",
    "    outputs:\n",
    "        model: Trained model\n",
    "        predict_test: predicted results\n",
    "        history: training history\n",
    "    '''\n",
    "    #build model\n",
    "    model, history = build_model_NAdam(Proc_X_train, Proc_Y_train,N_hidden_nodes,learning_rate,beta_1,beta_2, epsilon,Batch_size, Epochs,BEST_ACT_1,validation_split)\n",
    "    # Predict\n",
    "    \n",
    "    predict_test_scal = model.predict(Proc_X_test)\n",
    "    if 'ANN2' in model_type:\n",
    "      predict_test_scal = np.reshape(predict_test_scal, (get_N_output(Y_test), -1)).T\n",
    "    \n",
    "    predict_test = scaler_Y.inverse_transform(predict_test_scal)\n",
    "    return  model, predict_test_scal, predict_test, history\n",
    "def ActualTraining(model_type,Proc_X_train, Proc_Y_train,Proc_X_test,Y_test,OTHERTUNEDDICT,BEST_OPT,BEST_ACT_1,validation_split):\n",
    "    print('INFORMATION:')\n",
    "    print('Used otpimizer:',BEST_OPT)\n",
    "    print('Used hidden nodes activation function:',BEST_ACT_1)\n",
    "    D=OTHERTUNEDDICT\n",
    "    N_hidden_nodes=D['N_hidden_nodes']\n",
    "    learning_rate=D['learning_rate']\n",
    "    Batch_size=D['batch_size']\n",
    "    Epochs=D['nb_epoch']\n",
    "    print('Used N_hidden_nodes:',N_hidden_nodes)\n",
    "    print('Used learning_rate:',learning_rate)\n",
    "    print('Used Batch_size:',Batch_size)\n",
    "    print('Used Epochs:',Epochs)\n",
    "    if BEST_OPT=='SGD':\n",
    "        momentum=D['momentum']\n",
    "        nesterov=D['nesterov']\n",
    "        print('Used momentum:',momentum)\n",
    "        print('Used nesterov:',nesterov)\n",
    "        ANN, predict_test_scal,predict_test, history = get_result_SGD(model_type, Proc_X_train, Proc_Y_train, Proc_X_test,Y_test, N_hidden_nodes,learning_rate,momentum, nesterov,Batch_size, Epochs,BEST_ACT_1,validation_split)\n",
    "    if BEST_OPT=='RMSprop':\n",
    "        rho=D['rho']\n",
    "        momentum=D['momentum']\n",
    "        epsilon=D['epsilon']\n",
    "        print('Used rho:',rho)\n",
    "        print('Used momentum:',momentum)\n",
    "        print('Used epsilon:',epsilon)\n",
    "        ANN, predict_test_scal,predict_test, history =get_result_RMSprop(model_type, Proc_X_train, Proc_Y_train, Proc_X_test,Y_test,N_hidden_nodes,learning_rate,rho,momentum, epsilon,Batch_size, Epochs,BEST_ACT_1,validation_split)\n",
    "    if BEST_OPT=='Adagrad':\n",
    "        initial_accumulator_value=D['initial_accumulator_value']\n",
    "        epsilon=D['epsilon']\n",
    "        print('Used initial_accumulator_value:',initial_accumulator_value)\n",
    "        print('Used epsilon:',epsilon)\n",
    "        ANN, predict_test_scal,predict_test, history =get_result_Adagrad(model_type, Proc_X_train, Proc_Y_train, Proc_X_test,Y_test,N_hidden_nodes,learning_rate,initial_accumulator_value, epsilon,Batch_size, Epochs,BEST_ACT_1,validation_split)\n",
    "    if BEST_OPT=='Adadelta':\n",
    "        rho=D['rho']\n",
    "        epsilon=D['epsilon']\n",
    "        print('Used rho:',rho)\n",
    "        print('Used epsilon:',epsilon)\n",
    "        ANN, predict_test_scal,predict_test, history =get_result_Adadelta(model_type, Proc_X_train, Proc_Y_train, Proc_X_test,Y_test,N_hidden_nodes,learning_rate,rho, epsilon,Batch_size, Epochs,BEST_ACT_1,validation_split)\n",
    "    if BEST_OPT=='Adam':\n",
    "        beta_1=D['beta_1']\n",
    "        beta_2=D['beta_2']\n",
    "        epsilon=D['epsilon']\n",
    "        print('Used beta_1:',beta_1)\n",
    "        print('Used beta_2:',beta_2)\n",
    "        print('Used epsilon:',epsilon)\n",
    "        ANN, predict_test_scal,predict_test, history =get_result_Adam(model_type, Proc_X_train, Proc_Y_train, Proc_X_test,Y_test,N_hidden_nodes,learning_rate,beta_1,beta_2, epsilon,Batch_size, Epochs,BEST_ACT_1,validation_split)\n",
    "    if BEST_OPT=='Adamax':\n",
    "        beta_1=D['beta_1']\n",
    "        beta_2=D['beta_2']\n",
    "        epsilon=D['epsilon']\n",
    "        print('Used beta_1:',beta_1)\n",
    "        print('Used beta_2:',beta_2)\n",
    "        print('Used epsilon:',epsilon)\n",
    "        ANN, predict_test_scal,predict_test, history =get_result_Adamax(model_type, Proc_X_train, Proc_Y_train, Proc_X_test,Y_test,N_hidden_nodes,learning_rate,beta_1,beta_2, epsilon,Batch_size, Epochs,BEST_ACT_1,validation_split)\n",
    "    if BEST_OPT=='Nadam':\n",
    "        beta_1=D['beta_1']\n",
    "        beta_2=D['beta_2']\n",
    "        epsilon=D['epsilon']\n",
    "        print('Used beta_1:',beta_1)\n",
    "        print('Used beta_2:',beta_2)\n",
    "        print('Used epsilon:',epsilon)\n",
    "        ANN, predict_test_scal,predict_test, history =get_result_NAdam(model_type, Proc_X_train, Proc_Y_train, Proc_X_test,Y_test,N_hidden_nodes,learning_rate,beta_1,beta_2, epsilon,Batch_size, Epochs,BEST_ACT_1,validation_split)\n",
    "    return ANN, predict_test_scal,predict_test, history\n",
    "# Import dataset\n",
    "def Train_Model(training_dict):\n",
    "    warnings.filterwarnings('ignore')\n",
    "    seed_tensorflow(42)\n",
    "    working_folder=training_dict['working_folder']\n",
    "    datapathway=training_dict['datapathway']\n",
    "    input_column_list=training_dict['input_column_list']\n",
    "    output_column_list=training_dict['output_column_list'] #column of ystart\n",
    "    yname='Value'      # name of the ultimate output array (HSCP value)\n",
    "    yparameter='OUTPUT'      #position here but it will be what is the parameter to be tuned (HSCP_name)\n",
    "    xcolumn=training_dict['xcolumn'] #name of inputs\n",
    "    ycolumn=training_dict['ycolumn']  #name of outputs (HSCP_name)\n",
    "    optimizer_search_list=['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "    activation_list1=['relu',LeakyReLU(alpha=0.3),'hard_sigmoid','elu','sigmoid','softmax']\n",
    "    train_test_split_test_size=training_dict['train_test_split_test_size']\n",
    "    validation_split=0.1\n",
    "    model_path=training_dict['model_path']\n",
    "    x,y=data_import(datapathway,input_column_list,output_column_list)\n",
    "    # Pre-processing\n",
    "    global Proc_X_train, Proc_Y_train\n",
    "    model_type, Proc_X_train, Proc_Y_train, Proc_X_test, Y_test = pre_processing(training_dict['model_type'], x, y,train_test_split_test_size,xcolumn,ycolumn,yparameter,yname)\n",
    "    \n",
    "    # Tuning\n",
    "    def tune_optimizer_ACT(Proc_X_train, Proc_Y_train,optimizer_search_list,activation_list1,optimizer_tuning_number_of_full_update=200,\n",
    "                       optimizer_tuning_number_sample_used_in_one_training=10):\n",
    "        model = KerasRegressor(build_fn=create_model, nb_epoch=optimizer_tuning_number_of_full_update, batch_size=optimizer_tuning_number_sample_used_in_one_training, verbose=0) \n",
    "        optimizer = optimizer_search_list # 200 hidden nodes\n",
    "        param_grid = dict(optimizer=optimizer,activation1=activation_list1) \n",
    "        grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1,verbose=1)\n",
    "        grid_result = grid.fit(Proc_X_train, Proc_Y_train)\n",
    "        global BEST_ACT_1,BEST_OPT\n",
    "        BEST_ACT_1=grid.best_params_['activation1']\n",
    "        BEST_OPT=grid.best_params_['optimizer']\n",
    "        print('tuned activation function for hidden layer:')\n",
    "        print('BEST_ACT_1:',BEST_ACT_1)\n",
    "        print('tuned optimizer')\n",
    "        print('BEST_OPT:',BEST_OPT)\n",
    "    def  create_model(optimizer='adam',activation1='relu'):\n",
    "        '''\n",
    "     Build a NN\n",
    "     inputs:\n",
    "        N_hidden_nodes: number of neurons\n",
    "        input_dim: number of inputs\n",
    "        N_outputs: number of outputs\n",
    "        l_rate: learning rate\n",
    "        Batch_size: Batch size\n",
    "     outputs:\n",
    "        model: Trained model\n",
    "        history: training history'''\n",
    "        model = keras.Sequential([\n",
    "        keras.layers.Dense(200, activation=activation1, input_shape=(Proc_X_train.shape[1],)),\n",
    "        keras.layers.Dense(get_N_output(Proc_Y_train),activation='linear')\n",
    "        ])\n",
    "        model.compile(loss='mse',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mse'])\n",
    "        model.summary()\n",
    "        return model\n",
    "    def get_hype_T_dict(BEST_OPT,BEST_ACT_1,optimizer_search_list):\n",
    "        SGD_TD= {'momentum':[0,0.2,0.4,0.6,0.8],\n",
    "                 'nesterov':[True,False]}\n",
    "        RMSprop_TD={'rho':[0.001,0.01,0.1,0.5,0.9],\n",
    "                    'momentum':[0,0.2,0.4,0.6,0.8],\n",
    "                    'epsilon':[1e-9,1e-8,1e-7,1e-6,1e-5]}\n",
    "        Adagrad_TD={'initial_accumulator_value':[0.001,0.01,0.1,0.5,0.9],\n",
    "                    'epsilon':[1e-9,1e-8,1e-7,1e-6,1e-5]}\n",
    "        Adadelta_TD={'rho':[0.001,0.01,0.1,0.5,0.9],\n",
    "                     'epsilon':[1e-9,1e-8,1e-7,1e-6,1e-5]}\n",
    "        Adam_TD={'beta_1':[0.8   , 0.8475, 0.895 , 0.9425, 0.99  ],\n",
    "                 'beta_2':[0.999  ],#set to defult bez of lack of time or the project\n",
    "                 'epsilon':[1e-9,1e-8,1e-7,1e-6,1e-5]}\n",
    "        Adamax_TD=Adam_TD\n",
    "        Nadam_TD=Adam_TD\n",
    "        TD=[SGD_TD,RMSprop_TD,Adagrad_TD,Adadelta_TD,Adam_TD,Adamax_TD,Nadam_TD]\n",
    "        hype_search_dict={'learning_rate':[0.001,0.01,0.1,0.2],\n",
    "                          'N_hidden_nodes':[100,500,1000,1500,2000],\n",
    "                          'batch_size':[1,8,16,31], \n",
    "                          'nb_epoch':[1000,2000,3000],\n",
    "                          'BEST_ACT_1':[BEST_ACT_1]}\n",
    "        TD=[SGD_TD,RMSprop_TD,Adagrad_TD,Adadelta_TD,Adam_TD,Adamax_TD,Nadam_TD]\n",
    "        for i, opt in enumerate(optimizer_search_list):\n",
    "            if BEST_OPT==opt:\n",
    "                hype_search_dict.update(TD[i])\n",
    "                return hype_search_dict\n",
    "    def create_model_2_SGD(N_hidden_nodes,learning_rate,momentum, nesterov,BEST_ACT_1):\n",
    "        '''\n",
    "        Build a NN\n",
    "        inputs:\n",
    "        N_hidden_nodes: number of neurons\n",
    "        input_dim: number of inputs\n",
    "        N_outputs: number of outputs\n",
    "        l_rate: learning rate\n",
    "        Batch_size: Batch size\n",
    "        outputs:\n",
    "        model: Trained model\n",
    "        history: training history\n",
    "        '''\n",
    "        model = keras.Sequential([\n",
    "            keras.layers.Dense(N_hidden_nodes, activation=BEST_ACT_1, input_shape=(Proc_X_train.shape[1],)),\n",
    "            keras.layers.Dense(get_N_output(Proc_Y_train),activation='linear')])\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=keras.optimizers.SGD(learning_rate=learning_rate,momentum=momentum,nesterov=nesterov),\n",
    "                      metrics=['mse'])\n",
    "        print(model.summary())\n",
    "        return model\n",
    "\n",
    "    def create_model_2_RMSprop(N_hidden_nodes,learning_rate,rho,momentum, epsilon,BEST_ACT_1):\n",
    "        '''\n",
    "        Build a NN\n",
    "        inputs:\n",
    "        N_hidden_nodes: number of neurons\n",
    "        input_dim: number of inputs\n",
    "        N_outputs: number of outputs\n",
    "        l_rate: learning rate\n",
    "        Batch_size: Batch size\n",
    "        outputs:\n",
    "        model: Trained model\n",
    "        history: training history\n",
    "        '''\n",
    "        model = keras.Sequential([\n",
    "            keras.layers.Dense(N_hidden_nodes, activation=BEST_ACT_1, input_shape=(Proc_X_train.shape[1],)),\n",
    "            keras.layers.Dense(get_N_output(Proc_Y_train),activation='linear')\n",
    "        ])\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=keras.optimizers.RMSprop(learning_rate=learning_rate,rho=rho,momentum=momentum,epsilon=epsilon),\n",
    "                      metrics=['mse'])\n",
    "        print(model.summary())\n",
    "        return model\n",
    "    \n",
    "    def create_model_2_Adagrad(N_hidden_nodes,learning_rate,initial_accumulator_value, epsilon,BEST_ACT_1):\n",
    "        '''\n",
    "        Build a NN\n",
    "        inputs:\n",
    "        N_hidden_nodes: number of neurons\n",
    "        input_dim: number of inputs\n",
    "        N_outputs: number of outputs\n",
    "        l_rate: learning rate\n",
    "        Batch_size: Batch size\n",
    "        outputs:\n",
    "        model: Trained model\n",
    "        history: training history\n",
    "        '''\n",
    "        model = keras.Sequential([\n",
    "            keras.layers.Dense(N_hidden_nodes, activation=BEST_ACT_1, input_shape=(Proc_X_train.shape[1],)),\n",
    "            keras.layers.Dense(get_N_output(Proc_Y_train),activation='linear')\n",
    "        ])\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=keras.optimizers.Adagrad(learning_rate=learning_rate,initial_accumulator_value=initial_accumulator_value,epsilon=epsilon),\n",
    "                      metrics=['mse'])\n",
    "        print(model.summary())\n",
    "        return model\n",
    "\n",
    "    def create_model_2_Adadelta(N_hidden_nodes,learning_rate,rho, epsilon,BEST_ACT_1):\n",
    "        '''\n",
    "        Build a NN\n",
    "        inputs:\n",
    "        N_hidden_nodes: number of neurons\n",
    "        input_dim: number of inputs\n",
    "        N_outputs: number of outputs\n",
    "        l_rate: learning rate\n",
    "        Batch_size: Batch size\n",
    "        outputs:\n",
    "        model: Trained model\n",
    "        history: training history\n",
    "        '''\n",
    "        model = keras.Sequential([\n",
    "            keras.layers.Dense(N_hidden_nodes, activation=BEST_ACT_1, input_shape=(Proc_X_train.shape[1],)),\n",
    "            keras.layers.Dense(get_N_output(Proc_Y_train),activation='linear')\n",
    "        ])\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=keras.optimizers.Adadelta(learning_rate=learning_rate,rho=rho,epsilon=epsilon),\n",
    "                      metrics=['mse'])\n",
    "        print(model.summary())\n",
    "        return model\n",
    "    def create_model_2_Adam(N_hidden_nodes,learning_rate,beta_1,beta_2, epsilon,BEST_ACT_1):\n",
    "        '''\n",
    "        Build a NN\n",
    "        inputs:\n",
    "            N_hidden_nodes: number of neurons\n",
    "            input_dim: number of inputs\n",
    "            N_outputs: number of outputs\n",
    "            l_rate: learning rate\n",
    "            Batch_size: Batch size\n",
    "        outputs:\n",
    "            model: Trained model\n",
    "            history: training history\n",
    "        '''\n",
    "        model = keras.Sequential([\n",
    "            keras.layers.Dense(N_hidden_nodes, activation=BEST_ACT_1, input_shape=(Proc_X_train.shape[1],)),\n",
    "            keras.layers.Dense(get_N_output(Proc_Y_train),activation='linear')\n",
    "        ])\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=keras.optimizers.Adam(learning_rate=learning_rate,beta_1=beta_1,beta_2=beta_2,epsilon=epsilon),\n",
    "                      metrics=['mse'])\n",
    "        print(model.summary())\n",
    "        return model\n",
    "    def create_model_2_Adamax(N_hidden_nodes,learning_rate,beta_1,beta_2, epsilon,BEST_ACT_1):\n",
    "        '''\n",
    "        Build a NN\n",
    "        inputs:\n",
    "        N_hidden_nodes: number of neurons\n",
    "        input_dim: number of inputs\n",
    "        N_outputs: number of outputs\n",
    "        l_rate: learning rate\n",
    "        Batch_size: Batch size\n",
    "        outputs:\n",
    "        model: Trained model\n",
    "        history: training history\n",
    "        '''\n",
    "        model = keras.Sequential([\n",
    "           keras.layers.Dense(N_hidden_nodes, activation=BEST_ACT_1, input_shape=(Proc_X_train.shape[1],)),\n",
    "           keras.layers.Dense(get_N_output(Proc_Y_train),activation='linear')])\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=keras.optimizers.Adamax(learning_rate=learning_rate,beta_1=beta_1,beta_2=beta_2,epsilon=epsilon),\n",
    "                      metrics=['mse'])\n",
    "        print(model.summary())\n",
    "        return model\n",
    "    def create_model_2_Nadam(N_hidden_nodes,learning_rate,beta_1,beta_2, epsilon,BEST_ACT_1):\n",
    "        '''\n",
    "    Build a NN\n",
    "    inputs:\n",
    "        N_hidden_nodes: number of neurons\n",
    "        input_dim: number of inputs\n",
    "        N_outputs: number of outputs\n",
    "        l_rate: learning rate\n",
    "        Batch_size: Batch size\n",
    "    outputs:\n",
    "        model: Trained model\n",
    "        history: training history\n",
    "    '''\n",
    "        model = keras.Sequential([\n",
    "             keras.layers.Dense(N_hidden_nodes, activation=BEST_ACT_1, input_shape=(Proc_X_train.shape[1],)),\n",
    "               keras.layers.Dense(get_N_output(Proc_Y_train),activation='linear')\n",
    "               ])\n",
    "        model.compile(loss='mse',\n",
    "                             optimizer=keras.optimizers.Nadam(learning_rate=learning_rate,beta_1=beta_1,beta_2=beta_2,epsilon=epsilon),\n",
    "                                           metrics=['mse'])\n",
    "        print(model.summary())\n",
    "        return model\n",
    "    def tune_other_para(BEST_OPT,BEST_ACT_1,optimizer_search_list):\n",
    "        if BEST_OPT=='SGD':\n",
    "            model = KerasRegressor(build_fn=create_model_2_SGD,verbose=2)\n",
    "        if BEST_OPT=='RMSprop':\n",
    "            model = KerasRegressor(build_fn=create_model_2_RMSprop,verbose=2)\n",
    "        if BEST_OPT=='Adagrad':\n",
    "            model = KerasRegressor(build_fn=create_model_2_Adagrad,verbose=2)\n",
    "        if BEST_OPT=='Adadelta':\n",
    "            model = KerasRegressor(build_fn=create_model_2_Adadelta,verbose=2)\n",
    "        if BEST_OPT=='Adam':\n",
    "            model = KerasRegressor(build_fn=create_model_2_Adam,verbose=2)\n",
    "        if BEST_OPT=='Adamax':\n",
    "            model = KerasRegressor(build_fn=create_model_2_Adamax,verbose=2)\n",
    "        if BEST_OPT=='Nadam':\n",
    "            model = KerasRegressor(build_fn=create_model_2_Nadam,verbose=2)\n",
    "        param_grid = get_hype_T_dict(BEST_OPT,BEST_ACT_1,optimizer_search_list)\n",
    "        print('Tuning hyperparameters:')\n",
    "        print(param_grid)\n",
    "        grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1,verbose=2)\n",
    "        grid_result = grid.fit(Proc_X_train, Proc_Y_train)\n",
    "        global OTHERTUNEDDICT\n",
    "        OTHERTUNEDDICT=grid.best_params_\n",
    "        print('tuned hyperparameters:')\n",
    "        B_N_hidden_nodes=grid.best_params_['N_hidden_nodes']\n",
    "        B_learning_rate=grid.best_params_['learning_rate']\n",
    "        B_Batch_size=grid.best_params_['batch_size']\n",
    "        B_epochs=grid.best_params_['nb_epoch']\n",
    "        print('B_N_hidden_nodes:',B_N_hidden_nodes)\n",
    "        print('B_learning_rate:',B_learning_rate)\n",
    "        print('B_Batch_size:',B_Batch_size)\n",
    "        print('B_epochs:',B_epochs)\n",
    "        if BEST_OPT=='SGD':\n",
    "            B_momentum_SGD=grid.best_params_['momentum']\n",
    "            B_nesterov_SGD=grid.best_params_['nesterov']\n",
    "            print('B_momentum_SGD:',B_momentum_SGD)\n",
    "            print('B_nesterov_SGD:',B_nesterov_SGD)\n",
    "        if BEST_OPT=='RMSprop':\n",
    "            B_rho_RMSprop=grid.best_params_['rho']\n",
    "            B_momentum_RMSprop=grid.best_params_['momentum']\n",
    "            B_epsilon_RMSprop=grid.best_params_['epsilon']\n",
    "            print('B_rho_RMSprop:',B_rho_RMSprop)\n",
    "            print('B_momentum_RMSprop:',B_momentum_RMSprop)\n",
    "            print('B_epsilon_RMSprop:',B_epsilon_RMSprop)\n",
    "        if BEST_OPT=='Adagrad':\n",
    "            B_initial_accumulator_value_Adagrad=grid.best_params_['initial_accumulator_value']\n",
    "            B_epsilon_Adagrad=grid.best_params_['epsilon']\n",
    "            print('B_initial_accumulator_value_Adagrad:',B_initial_accumulator_value_Adagrad)\n",
    "            print('B_epsilon_Adagrad:',B_epsilon_Adagrad)\n",
    "        if BEST_OPT=='Adadelta':\n",
    "            B_rho_Adadelta=grid.best_params_['rho']\n",
    "            B_epsilon_Adadelta=grid.best_params_['epsilon']\n",
    "            print('B_rho_Adadelta:',B_rho_Adadelta)\n",
    "            print('B_epsilon_Adadelta:',B_epsilon_Adadelta)\n",
    "        if BEST_OPT =='Adam':\n",
    "            B_beta_1_Adam=grid.best_params_['beta_1']\n",
    "            B_beta_2_Adam=grid.best_params_['beta_2']\n",
    "            B_epsilon_Adam=grid.best_params_['epsilon']\n",
    "            print('B_beta_1_Adam:',B_beta_1_Adam)\n",
    "            print('B_beta_2_Adam:',B_beta_2_Adam)\n",
    "            print('B_epsilon_Adam:',B_epsilon_Adam)\n",
    "        if BEST_OPT =='Adamax':\n",
    "            B_beta_1_Adamax=grid.best_params_['beta_1']\n",
    "            B_beta_2_Adamax=grid.best_params_['beta_2']\n",
    "            B_epsilon_Adamax=grid.best_params_['epsilon']\n",
    "            print('B_beta_1_Adamax:',B_beta_1_Adamax)\n",
    "            print('B_beta_2_Adamax:',B_beta_2_Adamax)\n",
    "            print('B_epsilon_Adamax:',B_epsilon_Adamax)\n",
    "        if BEST_OPT =='Nadam':\n",
    "            B_beta_1_Nadam=grid.best_params_['beta_1']\n",
    "            B_beta_2_Nadam=grid.best_params_['beta_2']\n",
    "            B_epsilon_Nadam=grid.best_params_['epsilon']\n",
    "            print('B_beta_1_Nadam:',B_beta_1_Nadam)\n",
    "            print('B_beta_2_Nadam:',B_beta_2_Nadam)\n",
    "            print('B_epsilon_Nadam:',B_epsilon_Nadam)\n",
    "    def Save_SC(scaler_X,scaler_Y,working_folder):\n",
    "        pickle.dump(scaler_X, open(working_folder+r\"\\scaler_X.pkl\",'wb'))\n",
    "        pickle.dump(scaler_Y, open(working_folder+r\"\\scaler_Y.pkl\",'wb'))\n",
    "        print('done saving SC')\n",
    "    tune_optimizer_ACT(Proc_X_train, Proc_Y_train,optimizer_search_list,activation_list1,optimizer_tuning_number_of_full_update=200,\n",
    "                       optimizer_tuning_number_sample_used_in_one_training=10)\n",
    "    tune_other_para(BEST_OPT,BEST_ACT_1,optimizer_search_list)\n",
    "    ANN, predict_test_scal,predict_test, history=ActualTraining(model_type,Proc_X_train, Proc_Y_train,Proc_X_test,Y_test,OTHERTUNEDDICT=OTHERTUNEDDICT,BEST_OPT=BEST_OPT,BEST_ACT_1=BEST_ACT_1,validation_split=validation_split)\n",
    "    print_result(Y_test=Y_test,predict_test_scal=predict_test_scal,predict_test=predict_test,scaler_Y=scaler_Y, history=history)        \n",
    "    save_model(model_path=model_path,ANN=ANN)\n",
    "    Save_SC(scaler_X,scaler_Y,working_folder)\n",
    "    return ANN, scaler_X,scaler_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ebb7ffa-5ce6-4c3b-8c9b-cf31a87d1f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Use_Model(ANN_loc,SX_loc,SY_loc,input_list):\n",
    "    model=keras.models.load_model(ANN_loc)\n",
    "    SX=pickle.load(open(SX_loc,'rb'))\n",
    "    SY=pickle.load(open(SY_loc,'rb'))\n",
    "    input_S=SX.transform(np.array([input_list]))\n",
    "    answer_S=model.predict(input_S)\n",
    "    answer=SY.inverse_transform(answer_S)\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b149f39a-bcc7-4960-b0b9-e0754c74b9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dict= {'working_folder':r\"C:\\Users\\wingt\\OneDrive\\Desktop\\Individual Project ML\\1b_fusion_zone\\1b_x_fb\",\n",
    "                'datapathway':r\"C:\\Users\\wingt\\OneDrive\\Desktop\\Individual Project ML\\1b_fusion_zone\\1b_x_fb\\dataset_1b(Sheet1).csv\",\n",
    "                'input_column_list':[1,2,3,4,5,6],\n",
    "                'output_column_list':[7],\n",
    "                'xcolumn':['heat', 'speed', 'ra','rl','rv','y_fb'],\n",
    "                'ycolumn':['x_fb'],\n",
    "                'train_test_split_test_size':0.2,\n",
    "                'model_type':'ANN1'}\n",
    "model_path=r\"C:\\Users\\wingt\\OneDrive\\Desktop\\Individual Project ML\\1b_fusion_zone\\1b_x_fb\\saved_model\"\n",
    "training_dict.update({'model_path':model_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1711d43a-71c9-4245-bb72-1838cc2e55b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 42 candidates, totalling 210 fits\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 200)               1400      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 1,601\n",
      "Trainable params: 1,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "tuned activation function for hidden layer:\n",
      "BEST_ACT_1: relu\n",
      "tuned optimizer\n",
      "BEST_OPT: Adam\n",
      "Tuning hyperparameters:\n",
      "{'learning_rate': [0.001, 0.01, 0.1, 0.2], 'N_hidden_nodes': [100, 500, 1000, 1500, 2000], 'batch_size': [1, 8, 16, 31], 'nb_epoch': [1000, 2000, 3000], 'BEST_ACT_1': ['relu'], 'beta_1': [0.8, 0.8475, 0.895, 0.9425, 0.99], 'beta_2': [0.999], 'epsilon': [1e-09, 1e-08, 1e-07, 1e-06, 1e-05]}\n",
      "Fitting 5 folds for each of 6000 candidates, totalling 30000 fits\n"
     ]
    }
   ],
   "source": [
    "ANN_1a, scaler_X_1a,scaler_Y_1a=Train_Model(training_dict=training_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "466571e6-885e-4e31-b0f6-14b6b953630b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[408.23285]]\n"
     ]
    }
   ],
   "source": [
    "ANN_loc=r\"C:\\Users\\wingt\\OneDrive\\Desktop\\Individual Project ML\\1a_near_field\\saved_model\"\n",
    "SX_loc=r\"C:\\Users\\wingt\\OneDrive\\Desktop\\Individual Project ML\\1a_near_field\\scaler_X.pkl\"\n",
    "SY_loc=r\"C:\\Users\\wingt\\OneDrive\\Desktop\\Individual Project ML\\1a_near_field\\scaler_Y.pkl\"\n",
    "input_list=[1920,1.4,31,21,18]\n",
    "Use_Model(ANN_loc,SX_loc,SY_loc,input_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71760995-4d3e-41f6-aaba-75cd2dcca9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "1920,1.4,31,21,18,408.57"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
